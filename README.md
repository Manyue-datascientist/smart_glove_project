 ğŸ›ï¸Smart Glove for Visually Impaired Shopping Assistance
ğŸ“Œ Project Overview
This project is designed to assist visually impaired individuals in navigating stores and locating products using AI-powered object detection and hand guidance.

âœ” Uses YOLOv8 Nano for real-time object detection & segmentation.
âœ” Tracks hand movement using MediaPipe & IMU sensors.
âœ” Provides audio guidance & will integrate haptic feedback via a soft robotic glove.
âœ” Designed for real-world deployment on Raspberry Pi & Jetson Nano.

ğŸ¯ Features
âœ… Object Detection: Identify and locate store products (e.g., eggs, milk, etc.).
âœ… Hand Tracking: Estimate hand position relative to the detected object.
âœ… Real-Time Guidance: Assist users via audio feedback (voice commands).
âœ… Future Expansion: Integrate haptic glove for direct hand guidance.

ğŸ› ï¸ Tech Stack
ğŸ”¹ Software
Computer Vision: YOLOv8 Nano (Ultralytics)
Hand Tracking: MediaPipe, OpenCV, IMU Sensors (MPU6050)
Voice Feedback: TTS (Text-to-Speech) for real-time guidance
Model Deployment: ONNX, TensorRT (for Raspberry Pi)
API Interface: Flask (for future expansion)
ğŸ”¹ Hardware (Upcoming)
Processing Unit: Raspberry Pi 5 + AI HAT (13 TOPS NPU)
Camera: OAK-D or Orbbec Astra+
Actuators: Soft Pneumatic Actuators for Glove Feedback
Microcontroller: Arduino Nano 33 BLE for controlling actuators
Sensors: IMU (MPU6050) for hand movement detection

ğŸš€ Project Structure
 
smart_glove_project/
â”œâ”€â”€ src/                # Main source code
â”‚   â”œâ”€â”€ detection/      # Object detection & segmentation
â”‚   â”œâ”€â”€ hand_tracking/  # Hand tracking system
â”‚   â”œâ”€â”€ guidance/       # Audio & haptic feedback
â”‚   â”œâ”€â”€ hardware/       # Raspberry Pi / Arduino interface
â”‚   â”œâ”€â”€ utils/          # Common utilities
â”‚   â”œâ”€â”€ api/            # API Interface (future expansion)
â”œâ”€â”€ models/             # Trained YOLO models
â”œâ”€â”€ data/               # Datasets & annotations
â”œâ”€â”€ configs/            # Config files
â”œâ”€â”€ tests/              # Unit tests
â”œâ”€â”€ scripts/            # Training & deployment scripts
â”œâ”€â”€ README.md           # Project documentation
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ main.py             # Entry point for the system

ğŸ“Œ Setup Instructions
1ï¸âƒ£ Clone the Repository
 
git clone https://github.com/Manyue-datascientist/smart_glove_project
cd smart_glove_project
2ï¸âƒ£ Create & Activate Virtual Environment
 
python -m venv venv
source venv/bin/activate  # (Windows: venv\Scripts\activate)
3ï¸âƒ£ Install Dependencies
 
pip install -r requirements.txt
4ï¸âƒ£ Run Object Detection
 
python src/detection/yolo_detector.py
ğŸ‘¥ Contributors
[Manyu Javvadi](https://www.linkedin.com/in/manyue-javvadi-datascientist/) â€“ Project Lead

ğŸ“Œ Future Plans
âœ… Implement object detection & voice-based navigation.
ğŸ”œ Integrate haptic feedback via soft robotics.
ğŸ”œ Deploy on Raspberry Pi & Jetson Nano for real-world testing.